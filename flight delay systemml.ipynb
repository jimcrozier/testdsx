{
    "nbformat_minor": 0, 
    "metadata": {
        "kernelspec": {
            "name": "scala", 
            "display_name": "Scala 2.10 with Spark 1.6", 
            "language": "scala"
        }, 
        "language_info": {
            "name": "scala"
        }
    }, 
    "cells": [
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "# Flight Delay Prediction Demo Using SystemML"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "This notebook is based on datascientistworkbench.com's tutorial notebook for predicting flight delay."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Loading SystemML "
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "To use one of the released version, use \"%AddDeps org.apache.systemml systemml 0.9.0-incubating\". To use nightly build, \"%AddJar https://sparktc.ibmcloud.com/repo/latest/SystemML.jar\"\n\nOr you provide SystemML.jar and dependency through commandline when starting the notebook (for example: --packages com.databricks:spark-csv_2.10:1.4.0 --jars SystemML.jar)"
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Using cached version of SystemML.jar\n"
                }
            ], 
            "source": "%AddJar https://sparktc.ibmcloud.com/repo/latest/SystemML.jar", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 1
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Use Spark's CSV package for loading the CSV file"
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": ":: loading settings :: url = jar:file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/spark-assembly-1.6.0-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n:: resolving dependencies :: com.ibm.spark#spark-kernel;working [not transitive]\n\tconfs: [default]\n\tfound com.databricks#spark-csv_2.10;1.4.0 in central\ndownloading https://repo1.maven.org/maven2/com/databricks/spark-csv_2.10/1.4.0/spark-csv_2.10-1.4.0.jar ...\n\t[SUCCESSFUL ] com.databricks#spark-csv_2.10;1.4.0!spark-csv_2.10.jar (66ms)\n:: resolution report :: resolve 459ms :: artifacts dl 70ms\n\t:: modules in use:\n\tcom.databricks#spark-csv_2.10;1.4.0 from central in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   1   |   1   |   1   |   0   ||   1   |   1   |\n\t---------------------------------------------------------------------\n:: retrieving :: com.ibm.spark#spark-kernel\n\tconfs: [default]\n\t1 artifacts copied, 0 already retrieved (153kB/8ms)\n"
                }
            ], 
            "source": "%AddDeps com.databricks spark-csv_2.10 1.4.0", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 2
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "## Import Data"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Download the airline dataset from stat-computing.org if not already downloaded"
        }, 
        {
            "outputs": [], 
            "source": "import sys.process._\nimport java.net.URL\nimport java.io.File\nval url = \"http://stat-computing.org/dataexpo/2009/2007.csv.bz2\"\nval localFilePath = \"airline2007.csv.bz2\"\nif(!new java.io.File(localFilePath).exists) {\n    new URL(url) #> new File(localFilePath) !!\n}", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 3
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Load the dataset into DataFrame using Spark CSV package"
        }, 
        {
            "outputs": [], 
            "source": "import org.apache.spark.sql.SQLContext\nimport org.apache.spark.storage.StorageLevel\nval sqlContext = new SQLContext(sc)\nval fmt = sqlContext.read.format(\"com.databricks.spark.csv\")\nval opt = fmt.options(Map(\"header\"->\"true\", \"inferSchema\"->\"true\"))\nval airline = opt.load(localFilePath).na.replace( \"*\", Map(\"NA\" -> \"0.0\") )", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 4
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- Year: integer (nullable = true)\n |-- Month: integer (nullable = true)\n |-- DayofMonth: integer (nullable = true)\n |-- DayOfWeek: integer (nullable = true)\n |-- DepTime: string (nullable = true)\n |-- CRSDepTime: integer (nullable = true)\n |-- ArrTime: string (nullable = true)\n |-- CRSArrTime: integer (nullable = true)\n |-- UniqueCarrier: string (nullable = true)\n |-- FlightNum: integer (nullable = true)\n |-- TailNum: string (nullable = true)\n |-- ActualElapsedTime: string (nullable = true)\n |-- CRSElapsedTime: string (nullable = true)\n |-- AirTime: string (nullable = true)\n |-- ArrDelay: string (nullable = true)\n |-- DepDelay: string (nullable = true)\n |-- Origin: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- Distance: integer (nullable = true)\n |-- TaxiIn: integer (nullable = true)\n |-- TaxiOut: integer (nullable = true)\n |-- Cancelled: integer (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: integer (nullable = true)\n |-- CarrierDelay: integer (nullable = true)\n |-- WeatherDelay: integer (nullable = true)\n |-- NASDelay: integer (nullable = true)\n |-- SecurityDelay: integer (nullable = true)\n |-- LateAircraftDelay: integer (nullable = true)\n\n"
                }
            ], 
            "source": "airline.printSchema", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 5
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Data Exploration\nWhich airports have the most delays?"
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------+---------+------------------+\n|Origin|conFlight|             delay|\n+------+---------+------------------+\n|   PIR|        4|              45.5|\n|   ACK|      314|45.296178343949045|\n|   SOP|      195| 34.02051282051282|\n|   HHH|      997| 22.58776328986961|\n|   MCN|      992|22.496975806451612|\n|   AKN|      235|21.123404255319148|\n|   CEC|     1055|20.807582938388627|\n|   GNV|     1927| 20.69797612869746|\n|   EYW|     1052|20.224334600760457|\n|   ACY|      735|20.141496598639456|\n|   SPI|     1745|19.545558739255014|\n|   GST|       90|19.233333333333334|\n|   EWR|   154113|18.800853918877706|\n|   BRW|      726| 18.02754820936639|\n|   AGS|     2286|17.728346456692915|\n|   ORD|   375784|17.695756072637472|\n|   TRI|     1207| 17.63628831814416|\n|   SBN|     5128|17.505850234009362|\n|   FAY|     2185| 17.48970251716247|\n|   PHL|   104063|17.067776250924922|\n+------+---------+------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "airline.registerTempTable(\"airline\")\nsqlContext.sql(\"\"\"SELECT Origin, count(*) conFlight, avg(DepDelay) delay\n                    FROM airline\n                    GROUP BY Origin\n                    ORDER BY delay DESC\"\"\").show", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 6
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Modeling: Logistic Regression\n\nPredict departure delays of greater than 15 of flights from JFK"
        }, 
        {
            "outputs": [], 
            "source": "sqlContext.udf.register(\"checkDelay\", (depDelay:String) => try { if(depDelay.toDouble > 15) 1.0 else 2.0 } catch { case e:Exception => 1.0 })\nval tempSmallAirlineData = sqlContext.sql(\"SELECT *, checkDelay(DepDelay) label FROM airline WHERE Origin = 'JFK'\").persist(StorageLevel.MEMORY_AND_DISK)\nval popularDest = tempSmallAirlineData.select(\"Dest\").map(y => (y.get(0).toString, 1)).reduceByKey(_ + _).filter(_._2 > 1000).collect.toMap\nsqlContext.udf.register(\"onlyUsePopularDest\", (x:String) => popularDest.contains(x))\ntempSmallAirlineData.registerTempTable(\"tempAirline\")\nval smallAirlineData = sqlContext.sql(\"SELECT * FROM tempAirline WHERE onlyUsePopularDest(Dest)\")\n\nval datasets = smallAirlineData.randomSplit(Array(0.7, 0.3))\nval trainDataset = datasets(0).cache\nval testDataset = datasets(1).cache\ntrainDataset.count\ntestDataset.count", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 7
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### Feature selection"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Encode the destination using one-hot encoding and include the columns Year, Month, DayofMonth, DayOfWeek, Distance"
        }, 
        {
            "outputs": [], 
            "source": "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, VectorAssembler}\n\nval indexer = new StringIndexer().setInputCol(\"Dest\").setOutputCol(\"DestIndex\") // .setHandleInvalid(\"skip\") // Only works on Spark 1.6 or later\nval encoder = new OneHotEncoder().setInputCol(\"DestIndex\").setOutputCol(\"DestVec\")\nval assembler = new VectorAssembler().setInputCols(Array(\"Year\",\"Month\",\"DayofMonth\",\"DayOfWeek\",\"Distance\",\"DestVec\")).setOutputCol(\"features\")", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 8
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### Build the model: Use SystemML's MLPipeline wrapper. \n\nThis wrapper invokes MultiLogReg.dml (for training) and GLM-predict.dml (for prediction). These DML algorithms are available at https://github.com/apache/incubator-systemml/tree/master/scripts/algorithms"
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\nWelcome to Apache SystemML!\n\nBEGIN MULTINOMIAL LOGISTIC REGRESSION SCRIPT\nReading X...\nReading Y...\n-- Initially:  Objective = 56210.77060750876,  Gradient Norm = 4.4526526969600074E7,  Trust Delta = 0.001024586722033724\n-- Outer Iteration 1: Had 1 CG iterations\n   -- Obj.Reduction:  Actual = 9231.806553563853,  Predicted = 8882.93092452256  (A/P: 1.0393),  Trust Delta = 4.153060819933389E-4\n   -- New Objective = 46978.96405394491,  Beta Change Norm = 3.989950049590561E-4,  Gradient Norm = 3495966.22664141\n \n-- Outer Iteration 2: Had 2 CG iterations\n   -- Obj.Reduction:  Actual = 114.78674011961266,  Predicted = 112.77285709513916  (A/P: 1.0179),  Trust Delta = 4.153060819933389E-4\n   -- New Objective = 46864.1773138253,  Beta Change Norm = 1.1145435198192975E-4,  Gradient Norm = 90515.83715717653\nTermination / Convergence condition satisfied.\n"
                }
            ], 
            "source": "import org.apache.spark.ml.Pipeline\nimport org.apache.sysml.api.ml.LogisticRegression\n\nval lr = new LogisticRegression(\"log\", sc).setRegParam(1e-4).setTol(1e-2).setMaxInnerIter(0).setMaxOuterIter(100)\n\nval pipeline = new Pipeline().setStages(Array(indexer, encoder, assembler, lr))\nval model = pipeline.fit(trainDataset)", 
            "metadata": {
                "scrolled": true, 
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 9
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### Evaluate the model \n\nOutput RMS error on test data"
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------+-------------+\n|prediction|OriginalLabel|\n+----------+-------------+\n|       2.0|          2.0|\n|       2.0|          2.0|\n|       2.0|          2.0|\n|       2.0|          1.0|\n|       2.0|          2.0|\n|       2.0|          1.0|\n|       2.0|          1.0|\n|       2.0|          2.0|\n|       2.0|          1.0|\n|       2.0|          2.0|\n|       2.0|          2.0|\n|       2.0|          2.0|\n|       2.0|          2.0|\n|       2.0|          1.0|\n|       2.0|          1.0|\n|       2.0|          1.0|\n|       2.0|          2.0|\n|       2.0|          1.0|\n|       2.0|          1.0|\n|       2.0|          1.0|\n+----------+-------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "val predictions = model.transform(testDataset.withColumnRenamed(\"label\", \"OriginalLabel\"))\npredictions.select(\"prediction\", \"OriginalLabel\").show\nsqlContext.udf.register(\"square\", (x:Double) => Math.pow(x, 2.0))", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 10
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+------------------+\n|               _c0|\n+------------------+\n|0.5181495365908705|\n+------------------+\n\n"
                }
            ], 
            "source": "predictions.registerTempTable(\"predictions\")\nsqlContext.sql(\"SELECT sqrt(avg(square(OriginalLabel - prediction))) FROM predictions\").show", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 11
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### Perform k-fold cross-validation to tune the hyperparameters\n\nPerform cross-validation to tune the regularization parameter for Logistic regression."
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "BEGIN MULTINOMIAL LOGISTIC REGRESSION SCRIPT\nReading X...\nReading Y...\n-- Initially:  Objective = 28149.40014971994,  Gradient Norm = 2.219077877076751E7,  Trust Delta = 0.001024586722033724\n-- Outer Iteration 1: Had 1 CG iterations\n   -- Obj.Reduction:  Actual = 4575.598456876014,  Predicted = 4404.375311516109  (A/P: 1.0389),  Trust Delta = 4.130115632741793E-4\n   -- New Objective = 23573.801692843925,  Beta Change Norm = 3.9695545226363187E-4,  Gradient Norm = 1740806.9734678708\nTermination / Convergence condition satisfied.\nBEGIN MULTINOMIAL LOGISTIC REGRESSION SCRIPT\nReading X...\nReading Y...\n-- Initially:  Objective = 28149.40014971994,  Gradient Norm = 2.219077877076751E7,  Trust Delta = 0.001024586722033724\n-- Outer Iteration 1: Had 1 CG iterations\n   -- Obj.Reduction:  Actual = 4575.598456884985,  Predicted = 4404.375311523911  (A/P: 1.0389),  Trust Delta = 4.1301156327499524E-4\n   -- New Objective = 23573.801692834953,  Beta Change Norm = 3.969554522643349E-4,  Gradient Norm = 1740806.973475843\n \n-- Outer Iteration 2: Had 2 CG iterations\n   -- Obj.Reduction:  Actual = 62.61026798612511,  Predicted = 61.46039827317617  (A/P: 1.0187),  Trust Delta = 4.1301156327499524E-4\n   -- New Objective = 23511.19142484883,  Beta Change Norm = 1.2289959130971033E-4,  Gradient Norm = 49004.683058685594\n \n-- Outer Iteration 3: Had 2 CG iterations\n   -- Obj.Reduction:  Actual = 0.05569911524798954,  Predicted = 0.05566225038355237  (A/P: 1.0007),  Trust Delta = 4.1301156327499524E-4\n   -- New Objective = 23511.13572573358,  Beta Change Norm = 4.054800580797784E-6,  Gradient Norm = 3726.2308501944885\nTermination / Convergence condition satisfied.\nBEGIN MULTINOMIAL LOGISTIC REGRESSION SCRIPT\nReading X...\nReading Y...\n-- Initially:  Objective = 28149.40014971994,  Gradient Norm = 2.219077877076751E7,  Trust Delta = 0.001024586722033724\n-- Outer Iteration 1: Had 1 CG iterations\n   -- Obj.Reduction:  Actual = 4575.598456885076,  Predicted = 4404.375311523989  (A/P: 1.0389),  Trust Delta = 4.130115632750036E-4\n   -- New Objective = 23573.801692834862,  Beta Change Norm = 3.96955452264342E-4,  Gradient Norm = 1740806.9734759233\n \n-- Outer Iteration 2: Had 2 CG iterations\n   -- Obj.Reduction:  Actual = 62.61026798613602,  Predicted = 61.46039827318738  (A/P: 1.0187),  Trust Delta = 4.130115632750036E-4\n   -- New Objective = 23511.191424848726,  Beta Change Norm = 1.2289959130973744E-4,  Gradient Norm = 49004.68305870026\n \n-- Outer Iteration 3: Had 2 CG iterations\n   -- Obj.Reduction:  Actual = 0.05569911524798954,  Predicted = 0.055662250383595235  (A/P: 1.0007),  Trust Delta = 4.130115632750036E-4\n   -- New Objective = 23511.13572573348,  Beta Change Norm = 4.0548005808000325E-6,  Gradient Norm = 3726.2308501964685\n \n-- Outer Iteration 4: Had 3 CG iterations, trust bound REACHED\n   -- Obj.Reduction:  Actual = 1.445859593561181,  Predicted = 1.4458788314960183  (A/P: 1.0),  Trust Delta = 0.0016520462531000144\n   -- New Objective = 23509.689866139917,  Beta Change Norm = 4.130115632750036E-4,  Gradient Norm = 23831.828588442488\n \n-- Outer Iteration 5: Had 3 CG iterations, trust bound REACHED\n   -- Obj.Reduction:  Actual = 5.694055490766914,  Predicted = 5.693922827540441  (A/P: 1.0),  Trust Delta = 0.0066081850124000575\n   -- New Objective = 23503.99581064915,  Beta Change Norm = 0.0016520462531000146,  Gradient Norm = 3443.7288610241926\n \n-- Outer Iteration 6: Had 3 CG iterations, trust bound REACHED\n   -- Obj.Reduction:  Actual = 19.45245683736357,  Predicted = 19.452498997143362  (A/P: 1.0),  Trust Delta = 0.02643274004960023\n   -- New Objective = 23484.543353811787,  Beta Change Norm = 0.0066081850124000575,  Gradient Norm = 12965.765743801998\n \n-- Outer Iteration 7: Had 3 CG iterations, trust bound REACHED\n   -- Obj.Reduction:  Actual = 45.581585160838586,  Predicted = 45.542749390952594  (A/P: 1.0009),  Trust Delta = 0.03591812236578498\n   -- New Objective = 23438.96176865095,  Beta Change Norm = 0.026432740049600226,  Gradient Norm = 28636.211316855708\n \n-- Outer Iteration 8: Had 1 CG iterations\n   -- Obj.Reduction:  Actual = 0.009626374994695652,  Predicted = 0.009623816142655435  (A/P: 1.0003),  Trust Delta = 0.03591812236578498\n   -- New Objective = 23438.952142275954,  Beta Change Norm = 6.721431153143302E-7,  Gradient Norm = 1630.5560388652746\n \n-- Outer Iteration 9: Had 5 CG iterations, trust bound REACHED\n   -- Obj.Reduction:  Actual = 20.770371582162625,  Predicted = 20.7460412051269  (A/P: 1.0012),  Trust Delta = 0.03664250618657773\n   -- New Objective = 23418.18177069379,  Beta Change Norm = 0.03591812236578498,  Gradient Norm = 21735.58022803187\n \n-- Outer Iteration 10: Had 1 CG iterations\n   -- Obj.Reduction:  Actual = 0.0055487947320216335,  Predicted = 0.0055476797243223415  (A/P: 1.0002),  Trust Delta = 0.03664250618657773\n   -- New Objective = 23418.17622189906,  Beta Change Norm = 5.104698992270405E-7,  Gradient Norm = 713.5446652407655\n \n-- Outer Iteration 11: Had 5 CG iterations, trust bound REACHED\n   -- Obj.Reduction:  Actual = 6.483213094408711,  Predicted = 6.4867897122673135  (A/P: 0.9994),  Trust Delta = 0.05794192906837153\n   -- New Objective = 23411.69300880465,  Beta Change Norm = 0.03664250618657773,  Gradient Norm = 4232.454835625026\n \n-- Outer Iteration 12: Had 4 CG iterations\n   -- Obj.Reduction:  Actual = 2.7314115938643226,  Predicted = 2.728739705685696  (A/P: 1.001),  Trust Delta = 0.05794192906837153\n   -- New Objective = 23408.961597210786,  Beta Change Norm = 0.007553067234243498,  Gradient Norm = 2931.0237990745013\n \n-- Outer Iteration 13: Had 6 CG iterations, trust bound REACHED\n   -- Obj.Reduction:  Actual = 11.070476019762282,  Predicted = 11.062736995232182  (A/P: 1.0007),  Trust Delta = 0.1395859693684664\n   -- New Objective = 23397.891121191024,  Beta Change Norm = 0.05794192906837154,  Gradient Norm = 115082.65980341943\n \n-- Outer Iteration 14: Had 1 CG iterations\n   -- Obj.Reduction:  Actual = 0.15574584206478903,  Predicted = 0.15591476262133266  (A/P: 0.9989),  Trust Delta = 0.1395859693684664\n   -- New Objective = 23397.73537534896,  Beta Change Norm = 2.7096134706594607E-6,  Gradient Norm = 931.1520837084622\n \n-- Outer Iteration 15: Had 8 CG iterations, trust bound REACHED\n   -- Obj.Reduction:  Actual = 25.300939057149662,  Predicted = 25.30183814643172  (A/P: 1.0),  Trust Delta = 0.2739199264232279\n   -- New Objective = 23372.43443629181,  Beta Change Norm = 0.1395859693684664,  Gradient Norm = 57465.85315828573\n \n-- Outer Iteration 16: Had 2 CG iterations\n   -- Obj.Reduction:  Actual = 0.38148973068018677,  Predicted = 0.3812912151005322  (A/P: 1.0005),  Trust Delta = 0.2739199264232279\n   -- New Objective = 23372.05294656113,  Beta Change Norm = 1.341355784906472E-5,  Gradient Norm = 650.3542316228067\n \n-- Outer Iteration 17: Had 8 CG iterations, trust bound REACHED\n   -- Obj.Reduction:  Actual = 43.2444536031835,  Predicted = 43.24090342891277  (A/P: 1.0001),  Trust Delta = 0.6148484263466655\n   -- New Objective = 23328.808492957945,  Beta Change Norm = 0.27391992642322793,  Gradient Norm = 61697.93309600698\n \n-- Outer Iteration 18: Had 2 CG iterations\n   -- Obj.Reduction:  Actual = 0.43452618397714105,  Predicted = 0.43423954580444524  (A/P: 1.0007),  Trust Delta = 0.6148484263466655\n   -- New Objective = 23328.37396677397,  Beta Change Norm = 1.4355478387617242E-5,  Gradient Norm = 640.6203542616486\n \n-- Outer Iteration 19: Had 8 CG iterations, trust bound REACHED\n   -- Obj.Reduction:  Actual = 48.757621554948855,  Predicted = 48.96583240665677  (A/P: 0.9957),  Trust Delta = 0.8042103978998617\n   -- New Objective = 23279.61634521902,  Beta Change Norm = 0.6148484263466655,  Gradient Norm = 36469.73313869185\n \n-- Outer Iteration 20: Had 2 CG iterations\n   -- Obj.Reduction:  Actual = 0.038870042666530935,  Predicted = 0.03883548782343212  (A/P: 1.0009),  Trust Delta = 0.8042103978998617\n   -- New Objective = 23279.577475176353,  Beta Change Norm = 3.6025818846207718E-6,  Gradient Norm = 114.89691474593249\n \n-- Outer Iteration 21: Had 12 CG iterations\n   -- Obj.Reduction:  Actual = 10.498114643312874,  Predicted = 10.408866408989624  (A/P: 1.0086),  Trust Delta = 0.8042103978998617\n   -- New Objective = 23269.07936053304,  Beta Change Norm = 0.41274697697396057,  Gradient Norm = 12264.268110290539\n \n-- Outer Iteration 22: Had 2 CG iterations\n   -- Obj.Reduction:  Actual = 0.001981745575903915,  Predicted = 0.001981412595156327  (A/P: 1.0002),  Trust Delta = 0.8042103978998617\n   -- New Objective = 23269.077378787464,  Beta Change Norm = 4.3611925385698353E-7,  Gradient Norm = 5.997443113080835\nTermination / Convergence condition satisfied.\n"
                }, 
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 12, 
                    "data": {
                        "text/plain": "Name: java.lang.IllegalArgumentException\nMessage: Field \"rawPrediction\" does not exist.\nStackTrace: org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:212)\norg.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:212)\nscala.collection.MapLike$class.getOrElse(MapLike.scala:128)\nscala.collection.AbstractMap.getOrElse(Map.scala:58)\norg.apache.spark.sql.types.StructType.apply(StructType.scala:211)\norg.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:40)\norg.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:82)\norg.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:109)\norg.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:99)\nscala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\nscala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\norg.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:99)\n$line74.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:58)\n$line74.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:63)\n$line74.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:65)\n$line74.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:67)\n$line74.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:69)\n$line74.$read$$iwC$$iwC$$iwC.<init>(<console>:71)\n$line74.$read$$iwC$$iwC.<init>(<console>:73)\n$line74.$read$$iwC.<init>(<console>:75)\n$line74.$read.<init>(<console>:77)\n$line74.$read$.<init>(<console>:81)\n$line74.$read$.<clinit>(<console>)\n$line74.$eval$.<init>(<console>:7)\n$line74.$eval$.<clinit>(<console>)\n$line74.$eval.$print(<console>)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\njava.lang.reflect.Method.invoke(Method.java:507)\norg.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\norg.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\norg.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\norg.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\norg.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\ncom.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:296)\ncom.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:291)\ncom.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)\ncom.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)\ncom.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)\ncom.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.lang.Thread.run(Thread.java:785)"
                    }
                }
            ], 
            "source": "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n\nval crossval = new CrossValidator().setEstimator(pipeline).setEvaluator(new BinaryClassificationEvaluator)\nval paramGrid = new ParamGridBuilder().addGrid(lr.regParam, Array(0.1, 1e-3, 1e-6)).build()\ncrossval.setEstimatorParamMaps(paramGrid)\ncrossval.setNumFolds(2) // Setting k = 2\nval cvmodel = crossval.fit(trainDataset)", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 12
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 15, 
                    "data": {
                        "text/plain": "Name: Compile Error\nMessage: <console>:37: error: not found: value cvmodel\n              cvmodel\n              ^\nStackTrace: "
                    }
                }
            ], 
            "source": "cvmodel", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 15
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### Evaluate the cross-validated model"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "execution_count": 14, 
                    "data": {
                        "text/plain": "Name: Compile Error\nMessage: <console>:46: error: not found: value cvmodel\n         val cvpredictions = cvmodel.transform(testDataset.withColumnRenamed(\"label\", \"OriginalLabel\"))\n                             ^\nStackTrace: "
                    }
                }
            ], 
            "source": "val cvpredictions = cvmodel.transform(testDataset.withColumnRenamed(\"label\", \"OriginalLabel\"))\ncvpredictions.registerTempTable(\"cvpredictions\")\nsqlContext.sql(\"SELECT sqrt(avg(square(OriginalLabel - prediction))) FROM cvpredictions\").show", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 14
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Homework ;)\n\nRead http://apache.github.io/incubator-systemml/algorithms-classification.html#multinomial-logistic-regression and perform cross validation on other hyperparameters: for example: icpt, tol, maxOuterIter, maxInnerIter"
        }
    ], 
    "nbformat": 4
}